{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0cad49",
   "metadata": {},
   "source": [
    "# Performance of classifier on im2wav mixed audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f9e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85020eda",
   "metadata": {},
   "source": [
    "## Load self-trained best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83344ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configurations for the model\n",
    "class ResNetClassifier(nn.Module):\n",
    "    # Replacing original ResNet18's last layer with a custom classifier for multi-label classification\n",
    "    def __init__(self, num_classes, use_dropout=False, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Load a pretrained ResNet18 and modify the first and last layers\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # remove last layer(avg pool and fc) because we will use our own mlb classifier\n",
    "\n",
    "        layers = [\n",
    "            nn.Flatten(),# (B, 512, 1, 1) -> (B, 512)\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()# apply ReLU activation function to introduce non-linearity\n",
    "        ]\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(p=dropout_rate))\n",
    "        layers.append(nn.Linear(256, num_classes))# output logits for BCEWithLogitsLoss\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e94b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the audio data\n",
    "n_mels = 128\n",
    "n_fft = 1024\n",
    "hop_length = 512\n",
    "sample_rate = 16000\n",
    "target_duration_sec = 4\n",
    "target_length = sample_rate * target_duration_sec\n",
    "\n",
    "mel_transform = torch.nn.Sequential(\n",
    "    MelSpectrogram(\n",
    "        sample_rate=sample_rate, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length, \n",
    "        n_mels=n_mels\n",
    "    ),\n",
    "    AmplitudeToDB()\n",
    ")\n",
    "\n",
    "def preprocess_audio(filepath):\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "    \n",
    "    if waveform.shape[1] < target_length:\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, target_length - waveform.shape[1]))\n",
    "    elif waveform.shape[1] > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "    \n",
    "    mel_spec = mel_transform(waveform).squeeze(0)     # [128, Time]\n",
    "    mel_spec = mel_spec.unsqueeze(0).unsqueeze(0)     # -> [1, 1, 128, Time] for model\n",
    "    return mel_spec.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\st311_final\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\miniconda\\envs\\st311_final\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetClassifier(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['bird', 'cat', 'cow', 'dog', 'elephant', 'horse', 'lion', 'sheep']\n",
    "num_classes = len(class_names)\n",
    "test_folder = \"E:/LSE/ST311/ST311-Group-Project/audio_output/single_object_audios\"\n",
    "model = ResNetClassifier(num_classes=num_classes, use_dropout=False)\n",
    "model.load_state_dict(torch.load(\"E:/LSE/ST311/ST311-Group-Project/experiment/mixed_res/resnet_f1_0.9744.pth\"))\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260157e",
   "metadata": {},
   "source": [
    "# Performance on Single Animal Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23009897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_cat_bird1_bird1_0.wav: ['bird']\n",
      "0_cow_dog1_cow0_0.wav: []\n",
      "0_cow_horse1_cow1_0.wav: ['sheep']\n",
      "0_cow_sheep2_cow0_0.wav: ['cow']\n",
      "0_dog_bird1_bird1_0.wav: ['horse']\n",
      "0_dog_cat2_cat1_0.wav: ['horse']\n",
      "0_dog_horse2_dog1_0.wav: ['cow']\n",
      "0_elephant_bird1_bird1_0.wav: ['bird']\n",
      "0_elephant_lion1_elephant0_0.wav: ['lion']\n",
      "0_horse_sheep1_horse1_0.wav: ['dog']\n",
      "1_cat_bird1_cat0_0.wav: ['elephant']\n",
      "1_cow_dog1_dog1_0.wav: ['horse']\n",
      "1_cow_horse1_horse0_0.wav: ['cat', 'cow']\n",
      "1_cow_sheep2_sheep1_0.wav: ['cat']\n",
      "1_dog_bird1_dog0_0.wav: ['dog']\n",
      "1_dog_cat2_dog0_0.wav: ['dog']\n",
      "1_dog_horse2_horse0_0.wav: ['elephant']\n",
      "1_elephant_bird1_elephant0_0.wav: ['cow', 'sheep']\n",
      "1_elephant_lion1_lion0_0.wav: ['lion']\n",
      "1_horse_sheep1_sheep0_0.wav: ['cat', 'sheep']\n",
      "2_cat_bird2_bird1_0.wav: ['bird']\n",
      "2_cow_dog2_cow0_0.wav: ['dog', 'elephant']\n",
      "2_cow_horse2_cow0_0.wav: ['sheep']\n",
      "2_cow_sheep3_cow0_0.wav: ['dog']\n",
      "2_dog_cat4_cat0_0.wav: ['elephant']\n",
      "2_dog_horse3_dog1_0.wav: ['dog']\n",
      "2_elephant_lion3_elephant0_0.wav: ['sheep']\n",
      "2_horse_sheep2_bird2_0.wav: ['dog']\n",
      "3_cat_bird2_cat0_0.wav: ['cat']\n",
      "3_cow_dog2_dog1_0.wav: ['dog', 'elephant']\n",
      "3_cow_horse2_horse1_0.wav: ['horse']\n",
      "3_cow_sheep3_sheep1_0.wav: ['sheep']\n",
      "3_dog_cat4_dog1_0.wav: ['dog', 'elephant']\n",
      "3_dog_horse3_horse0_0.wav: ['cat', 'elephant']\n",
      "3_elephant_lion3_lion0_0.wav: ['lion']\n",
      "3_horse_sheep2_horse0_0.wav: ['cow', 'horse']\n",
      "4_cow_dog5_cow0_0.wav: ['cow', 'sheep']\n",
      "4_cow_horse3_cow0_0.wav: ['sheep']\n",
      "4_cow_sheep4_cow0_0.wav: ['horse']\n",
      "4_dog_cat6_cat1_0.wav: ['bird']\n",
      "4_dog_horse7_dog1_0.wav: ['dog']\n",
      "4_elephant_lion4_elephant0_0.wav: ['elephant']\n",
      "4_horse_sheep2_sheep1_0.wav: ['cow']\n",
      "5_cow_dog5_dog1_0.wav: ['dog']\n",
      "5_cow_horse3_horse1_0.wav: ['lion']\n",
      "5_cow_sheep4_sheep1_0.wav: ['elephant']\n",
      "5_dog_cat6_dog0_0.wav: ['horse']\n",
      "5_dog_horse7_horse0_0.wav: ['dog']\n",
      "5_elephant_lion4_lion0_0.wav: ['lion']\n",
      "6_cow_horse4_cow1_0.wav: ['cat']\n",
      "7_cow_horse4_horse0_0.wav: ['cat']\n",
      "\n",
      "Results saved in inference_results_single.csv\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.3\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(test_folder):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        path = os.path.join(test_folder, filename)\n",
    "        input_tensor = preprocess_audio(path)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).squeeze().cpu().numpy()\n",
    "            predicted_labels = [class_names[i] for i, p in enumerate(preds) if p]\n",
    "            results.append([filename] + predicted_labels)\n",
    "            print(f\"{filename}: {predicted_labels}\")\n",
    "\n",
    "# Save results as CSV\n",
    "csv_path = \"inference_results_single.csv\"\n",
    "with open(csv_path, \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"filename\"] + [f\"tag{i+1}\" for i in range(10)])  # header row\n",
    "    for row in results:\n",
    "        writer.writerow(row + [\"\"] * (11 - len(row)))  # pad if fewer than 10 tags\n",
    "\n",
    "print(f\"\\nResults saved in {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67ac0b",
   "metadata": {},
   "source": [
    "# Performance on Mixed Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c1aa1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed_cat_bird1.wav: ['bird']\n",
      "mixed_cat_bird2.wav: ['cat', 'sheep']\n",
      "mixed_cow_dog1.wav: ['dog', 'elephant', 'horse']\n",
      "mixed_cow_dog2.wav: ['dog', 'elephant']\n",
      "mixed_cow_dog5.wav: ['cow', 'elephant', 'sheep']\n",
      "mixed_cow_horse1.wav: ['sheep']\n",
      "mixed_cow_horse2.wav: ['cat', 'sheep']\n",
      "mixed_cow_horse3.wav: ['cow', 'lion']\n",
      "mixed_cow_horse4.wav: ['cat', 'elephant']\n",
      "mixed_cow_sheep2.wav: ['cow', 'sheep']\n",
      "mixed_cow_sheep3.wav: ['dog', 'sheep']\n",
      "mixed_cow_sheep4.wav: ['elephant']\n",
      "mixed_dog_bird1.wav: ['dog']\n",
      "mixed_dog_cat2.wav: ['dog']\n",
      "mixed_dog_cat4.wav: ['dog']\n",
      "mixed_dog_cat6.wav: ['elephant', 'horse']\n",
      "mixed_dog_horse2.wav: ['dog', 'elephant']\n",
      "mixed_dog_horse3.wav: ['dog']\n",
      "mixed_dog_horse7.wav: ['cat', 'dog']\n",
      "mixed_elephant_bird1.wav: ['bird']\n",
      "mixed_elephant_lion1.wav: ['lion']\n",
      "mixed_elephant_lion3.wav: ['dog', 'lion']\n",
      "mixed_elephant_lion4.wav: ['elephant']\n",
      "mixed_horse_sheep1.wav: ['cat', 'dog']\n",
      "mixed_horse_sheep2.wav: ['cat', 'cow', 'dog']\n",
      "\n",
      "Results saved in inference_results_01.csv\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "test_folder_mixed = \"E:/LSE/ST311/ST311-Group-Project/audio_output/mixed_test_audios\"\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(test_folder_mixed):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        path = os.path.join(test_folder_mixed, filename)\n",
    "        input_tensor = preprocess_audio(path)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).squeeze().cpu().numpy()\n",
    "            predicted_labels = [class_names[i] for i, p in enumerate(preds) if p]\n",
    "            results.append([filename] + predicted_labels)\n",
    "            print(f\"{filename}: {predicted_labels}\")\n",
    "\n",
    "# Save results as CSV\n",
    "csv_path = \"inference_results_01.csv\"\n",
    "with open(csv_path, \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"filename\"] + [f\"tag{i+1}\" for i in range(10)])  # header row\n",
    "    for row in results:\n",
    "        writer.writerow(row + [\"\"] * (11 - len(row)))  # pad if fewer than 10 tags\n",
    "\n",
    "print(f\"\\nResults saved in {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78987ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed_cat_bird1.wav: ['bird']\n",
      "mixed_cat_bird2.wav: ['cat']\n",
      "mixed_cow_dog1.wav: []\n",
      "mixed_cow_dog2.wav: ['dog', 'elephant']\n",
      "mixed_cow_dog5.wav: ['sheep']\n",
      "mixed_cow_horse1.wav: ['sheep']\n",
      "mixed_cow_horse2.wav: ['cat', 'sheep']\n",
      "mixed_cow_horse3.wav: ['lion']\n",
      "mixed_cow_horse4.wav: ['cat']\n",
      "mixed_cow_sheep2.wav: ['cow']\n",
      "mixed_cow_sheep3.wav: ['sheep']\n",
      "mixed_cow_sheep4.wav: ['elephant']\n",
      "mixed_dog_bird1.wav: ['dog']\n",
      "mixed_dog_cat2.wav: ['dog']\n",
      "mixed_dog_cat4.wav: ['dog']\n",
      "mixed_dog_cat6.wav: ['horse']\n",
      "mixed_dog_horse2.wav: ['dog', 'elephant']\n",
      "mixed_dog_horse3.wav: ['dog']\n",
      "mixed_dog_horse7.wav: ['dog']\n",
      "mixed_elephant_bird1.wav: ['bird']\n",
      "mixed_elephant_lion1.wav: ['lion']\n",
      "mixed_elephant_lion3.wav: ['lion']\n",
      "mixed_elephant_lion4.wav: ['elephant']\n",
      "mixed_horse_sheep1.wav: ['cat', 'dog']\n",
      "mixed_horse_sheep2.wav: ['cat']\n",
      "\n",
      "Results saved in inference_results_higherth.csv\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.3\n",
    "test_folder_mixed = \"E:/LSE/ST311/ST311-Group-Project/audio_output/mixed_test_audios\"\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(test_folder_mixed):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        path = os.path.join(test_folder_mixed, filename)\n",
    "        input_tensor = preprocess_audio(path)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).squeeze().cpu().numpy()\n",
    "            predicted_labels = [class_names[i] for i, p in enumerate(preds) if p]\n",
    "            results.append([filename] + predicted_labels)\n",
    "            print(f\"{filename}: {predicted_labels}\")\n",
    "\n",
    "# Save results as CSV\n",
    "csv_path = \"inference_results_higherth.csv\"\n",
    "with open(csv_path, \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"filename\"] + [f\"tag{i+1}\" for i in range(10)])  # header row\n",
    "    for row in results:\n",
    "        writer.writerow(row + [\"\"] * (11 - len(row)))  # pad if fewer than 10 tags\n",
    "\n",
    "print(f\"\\nResults saved in {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44e10b",
   "metadata": {},
   "source": [
    "## PANN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e220b0",
   "metadata": {},
   "source": [
    "We modified PANN's inference file and run the command in local terminal(Windows powershell). Results are saved in csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5543da",
   "metadata": {},
   "source": [
    "\n",
    "python pytorch/inference_modified.py `\n",
    "    --model_type=Cnn14_16k `\n",
    "    --checkpoint_path=E:/LSE/ST311/ST311-Group-Project/Cnn14_16k_mAP=0.438.pth `\n",
    "    --audio_folder=\"E:/LSE/ST311/ST311-Group-Project/audio_output/mixed_test_audios\" `\n",
    "    --cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe35db",
   "metadata": {},
   "source": [
    "# Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "319997bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3214\n",
      "Recall:    0.1800\n",
      "F1 Score:  0.2308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "csv_path = \"E:/LSE/ST311/ST311-Group-Project/inference_results_higherth.csv\"\n",
    "\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "all_labels_set = set()\n",
    "\n",
    "with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # gain the ground truth labels from filename\n",
    "        fname = row['filename']\n",
    "        parts = fname.replace(\".wav\", \"\").split(\"_\")[1:]  # delete 'mixed'\n",
    "        true_labels = set(parts)\n",
    "        \n",
    "        # prediction（tag1 ~ tag10）\n",
    "        pred_labels = set(filter(None, [row[f'tag{i}'] for i in range(1, 11)]))\n",
    "        \n",
    "        all_labels_set.update(true_labels)\n",
    "        all_labels_set.update(pred_labels)\n",
    "\n",
    "        y_true_all.append(true_labels)\n",
    "        y_pred_all.append(pred_labels)\n",
    "\n",
    "# turn into multi-hot vectors\n",
    "labels_list = sorted(list(all_labels_set))\n",
    "label_to_idx = {label: i for i, label in enumerate(labels_list)}\n",
    "\n",
    "def to_multihot(label_set):\n",
    "    vec = [0] * len(labels_list)\n",
    "    for label in label_set:\n",
    "        if label in label_to_idx:\n",
    "            vec[label_to_idx[label]] = 1\n",
    "    return vec\n",
    "\n",
    "y_true_vecs = [to_multihot(s) for s in y_true_all]\n",
    "y_pred_vecs = [to_multihot(s) for s in y_pred_all]\n",
    "\n",
    "\n",
    "precision = precision_score(y_true_vecs, y_pred_vecs, average='micro')\n",
    "recall = recall_score(y_true_vecs, y_pred_vecs, average='micro')\n",
    "f1 = f1_score(y_true_vecs, y_pred_vecs, average='micro')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dab1d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2034\n",
      "Recall:    0.0600\n",
      "F1 Score:  0.0927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "csv_path = \"E:/LSE/ST311/ST311-Group-Project/inference_results_single.csv\"\n",
    "\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "all_labels_set = set()\n",
    "\n",
    "with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # gain the ground truth labels from filename\n",
    "        fname = row['filename']\n",
    "        parts = fname.replace(\".wav\", \"\").split(\"_\")[1:]  # delete 'mixed'\n",
    "        true_labels = set(parts)\n",
    "        \n",
    "        # prediction（tag1 ~ tag10）\n",
    "        pred_labels = set(filter(None, [row[f'tag{i}'] for i in range(1, 11)]))\n",
    "        \n",
    "        all_labels_set.update(true_labels)\n",
    "        all_labels_set.update(pred_labels)\n",
    "\n",
    "        y_true_all.append(true_labels)\n",
    "        y_pred_all.append(pred_labels)\n",
    "\n",
    "# turn into multi-hot vectors\n",
    "labels_list = sorted(list(all_labels_set))\n",
    "label_to_idx = {label: i for i, label in enumerate(labels_list)}\n",
    "\n",
    "def to_multihot(label_set):\n",
    "    vec = [0] * len(labels_list)\n",
    "    for label in label_set:\n",
    "        if label in label_to_idx:\n",
    "            vec[label_to_idx[label]] = 1\n",
    "    return vec\n",
    "\n",
    "y_true_vecs = [to_multihot(s) for s in y_true_all]\n",
    "y_pred_vecs = [to_multihot(s) for s in y_pred_all]\n",
    "\n",
    "\n",
    "precision = precision_score(y_true_vecs, y_pred_vecs, average='micro')\n",
    "recall = recall_score(y_true_vecs, y_pred_vecs, average='micro')\n",
    "f1 = f1_score(y_true_vecs, y_pred_vecs, average='micro')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st311_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
